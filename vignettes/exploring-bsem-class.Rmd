---
title: "Exploring bsem class"
subtitle: "How evaluate bsem package outcomes"
author: 
  - name          : "Renato Panaro"
keywords          : "sem, posterior, diagram, summary, plot"
date: "`r Sys.Date()`"
output:
  html_document:
    theme: flatly
    toc: true
    toc_float:
      collapsed: true
      smooth_scroll: true
      toc_number: true
vignette: >
  %\VignetteIndexEntry{Exploring bsem class}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---

<style>
body {
text-align: justify
</style>

This vignette describes how to explore the estimates obtained with `bsem::sem`. In light of replication studies (e.g: Monte Carlo simulation study), we ran a former analysis to validate the proposed framework. We highly recommend the reader to take a look at [Get started](bsem.html) before reading this material as the general model described in the introduction of this vignette is presented next. 
  
## CFA

The main `bsem::sem` routine allows semi or full confirmatory factor analysis (CFA). Thus, at least one variable must be chosen to express each construct before running into the analysis. In other words, the user needs to specify at least one manifest variable that leads each block. In this way, semi-confirmatory FA might assist the researcher in evidencing the possible structures to be tested in a conceptual (full) CFA later on.

The data used in this example was generated using the function `bsem::simdata`, that internally produces artificial variances used to generate normally distributed errors. Also, factor scores (lambda) are generated from a standard normal distribution, and the factor loadings (alpha) from a uniform distribution with support ranging from 0.5 to 2. The factor loading direction (signal) is generated using a simple random sampling. A new artificial data set is wrapped in a list object and can be produced with the following command:

```{r, echo = F}
set.seed(2020)
```

```{r}
dt <- bsem::simdata(
  paths = NULL,
  exogenous = NULL
)
```

The `paths` and` exogenous` arguments must be defined as `NULL` to obtain a data set suitable for factor analysis, in which paths or exogenous variables can not be considered. Now, we can run `bsem::sem` to check whether the estimates retrieved are close to the real values of the parameters.

A data matrix and a list of blocks are the only mandatory arguments as they are necessary to represent the manifest variables related to each construct of the outer model. There are two ways to specify a ``sem`` model: 

1. the blocks, paths and exogenous elements are integer vectors referring to the variable (column) position;

2. named lists defined by the user with the factor and variable names, such that the automated print, summary, and plot functions will inherit it.

- In this example we go ahead with the first option, unnamed lists are internally named in both cases.

```{r}
dt$blocks
```

Note that the list is named with `c("F1", "F2", "F3", "F4", "F5")`, but the elements refer to column positioning, variable names would be internally converted into positioning if we had the first case.

- Descriptive statistics were based on ``rstan::monitor``:

```{r}
cfa <- bsem::sem(
  data = dt$data,
  blocks = dt$blocks
)

cfa
```

The descriptive statistics for the posterior factor loadings are shown in the R console inspired on the `rstan` fashion (based on ``rstan::monitor``). Besides, a warning is displayed to remind that the signals of the outer model loadings were not specified. Random initial values are assigned in this case. As a consequence, the direction of the loadings signals  might change, which means that the score rows and loading columns may vary. Remind that the real-valued product $\alpha \lambda$ can have more than one solution.

- The structural model can be easily viewed with the plotting routine `bsem::plot`:

```{r}
plot(cfa)
```

Another interesting visualization of the result is to compare the posterior factor loadings estimates, and also the score estimates, with the true values of these quantities using the `bsem::arrayplot` function. For example:

```{r}
gridExtra::grid.arrange(
  bsem::arrayplot(cfa$mean_lambda, main = "estimates", -6, 6),
  bsem::arrayplot(dt$real$lambda, main = "lambda (scores)", -6, 6)
)

gridExtra::grid.arrange(
  bsem::arrayplot(cfa$mean_alpha, main = "estimates", -6, 6),
  bsem::arrayplot(dt$real$alpha, main = "alpha (loadings)", -6, 6),
  layout_matrix = matrix(c(1, 1, 2, 2), ncol = 2)
)
```

From this example, we found that the estimates pattern is tightly close to the one generated with the true value of the parameters even when the signals are not passed. 

Remember each chain runs in a separate thread, thus, it is recommended to specify (default) the number of cores at least equal to the number of chains. Indeed, from our experience, an increase in the number of iterations raises the runtime, but considerably improves the interval and point estimates. 

## SEM

In addition to the CFA structure above, for the SEM we wil also consider paths and exogenous variables. The simulated data already described in the [Get started](bsem.html) vignette includes two additional kinds of relationships among observed and latent variables.

1. Exogenous variable regression - variables that are explained by some common factors, those variables are referred to as the exogenous variables.

2. Path modeling regression -  common factors that are explained by some other common factors,  this we will refer to as  a path between two latent variables.

Again, we can use the predefined routine `simdata`.

```{r}
dt <- bsem::simdata()
```

```{r}
sem <- bsem::sem(
  data = dt$data,
  blocks = dt$blocks,
  paths = dt$paths,
  exogenous = dt$exogenous
)
```

Similar outcomes can be also obtained for this model and we can one more time conclude that the model estimates are adequate under normality assumption for the errors:

```{r, echo = F}
gridExtra::grid.arrange(
  bsem::arrayplot(sem$mean_lambda, main = "estimates", -6, 6),
  bsem::arrayplot(dt$real$lambda, main = "lambda (scores)", -6, 6)
  )

gridExtra::grid.arrange(
  bsem::arrayplot(sem$mean_alpha, main = "estimates", -6, 6),
  bsem::arrayplot(dt$real$alpha, main = "alpha (loadings)", -6, 6),
  layout_matrix = matrix(c(1, 1, 2, 2), ncol = 2)
)
```

An analogous plot can be retrieved, but in addition to the previous structure, we can observe ellipsoids, boxes and also a circle. The circle represent the exogenous variable, while the ellipses represent the common factors and the boxes the manifest variables. The dashed lines refers to the regression coefficients and the solid ones to the CFA loading.

```{r, echo = F}
plot(sem)
```

That is not all, the first thing to do after fitting a Bayesian model is to check the posterior to assess whether the estimates are good to describe the proposed CFA or the SEM model.

## Graphical posterior analysis 

This section is an introduction on how to diagnose the adequacy of the estimates using the outputs of `cfa` and `semfit`. For example, we will analyze the CFA example.

### Traceplots and density plots

The posterior samples are returned with `cfa$posterior` and the posterior mean with `cfa$mean_alpha`. 

```{r}
names(cfa$posterior)
dim(cfa$posterior$alpha)
```

The dimension indicates that we have got $1000$ after-warmup iterations, 4 chains and 105 loadings. We select one the ones that we previously considered as manifest variables in the CFA.

```{r}
lnames <- rownames(cfa$mean_alpha)

  find <- paste0("alpha[", which(lnames %in% unlist(cfa$blocks[1:length(cfa$blocks)])), ",", 
                 rep(1:length(cfa$blocks), lengths(cfa$blocks)), "]"
                 )
  bayesplot::mcmc_trace(cfa$posterior$alpha[, , find]
                        )
```

```{r}
lnames <- rownames(cfa$mean_alpha)

  find <- paste0("alpha[", which(lnames %in% unlist(cfa$blocks[1:length(cfa$blocks)])), ",", 
                 rep(1:length(cfa$blocks), lengths(cfa$blocks)), "]"
                 )
  bayesplot::mcmc_dens(cfa$posterior$alpha[, , find]
                        )
```

The plots reveal that the chains are unimodal and have good mixing. The signals between chains might change as mentioned before, while extracting the posterior samples the routine internally compares opposite signal chains and adjust the direction between chains.

## Interval estimate

Two types of intervals can be obtained:  

### HPD interval  
- The narrowest (highest posterior density - HPD) credibility interval can be retrieved using `.$credint`. We have computed HPD intervals for the factor loadings (alpha), scores (lambda), and regression coefficients (beta).

```{r}
names(cfa$credint)
```

- This data can be used to plot interval estimates using two packages, `ggplot2` and `tidybayes`:

```{r, results = F, message = F, warning= F} 
library("ggplot2")
library("tidybayes")
```

- `dt` data.frame object has the mean loadings and the HPD interval lower and upper limits (`ll` and `lu`):

```{r}
dt <- data.frame(
  li = cfa$credint$alpha[, 1],
  lu = cfa$credint$alpha[, 2],
  m = c(cfa$mean_alpha)
)
```

- `lnames` and `snames` are used to find the loadings of the conceptual model (those that might not equal zero).

```{r}
lnames <- rownames(cfa$mean_alpha)
snames <- rownames(cfa$mean_lambda)
```

- `find` help us finding these values:
  
```{r}
find <- paste0("alpha[", which(lnames %in% unlist(cfa$blocks)), ",", rep(1:length(cfa$blocks), lengths(cfa$blocks)), "]")

dt <- dt[find, ]
```

- One of the options to plot the loadings HPD intervals chart is:
```{r}
ggplot(aes(y = find, x = m, xmin = li, xmax = lu), data = dt) +
  geom_pointintervalh() +
  theme_classic() +
  labs(
    title = paste("Latent variable", colnames(cfa$mean_alpha)[3]),
    x = "Highest posterior density interval",
    y = "variable"
  )
```

In the SEM example above, all intervals regarding the loading estimates from the conceptual relationships do not include zero. 

### Equal tails interval

Alternatively, it is possible to access equal tails credibility intevals using the `bayesplot` package:

````{r, message = F, message = F}
library("bayesplot")
```

- `find` help us finding the loading values that have been previously determined to estimate the latent scores:

```{r, eval =F}
find <- paste0("alpha[", which(lnames %in% unlist(cfa$blocks)), ",",
               rep(1:length(cfa$blocks), lengths(cfa$blocks)), "]")

dt <- dt[find, ]
```

- The equal tails intervals are:

````{r, warning = F}
gridExtra::grid.arrange(mcmc_areas(cfa$posterior$alpha[, , find]),
  mcmc_intervals(cfa$posterior$alpha[, , find]),
  layout_matrix = matrix(c(1, 1, 2, 2), ncol = 2)
)
````

In addition to the possibilities for intervals, other `mcmc_.` graphs are recommended, several options include histograms, violin plots, pair plots, and others. A good tutorial on how to analyze the posterior using `bayesplot` functions is available at the [bayesplot](https://mc-stan.org/users/interfaces/bayesplot) manual.


The data visualization tools have been covered in this document. More than the options covered here, there is also a `runshiny()` that can be useful to understand the model. The user can upload a .csv dataset and follow the proposed workflow. The code to launch the app is:

```{r, eval = F}
bsem::runShiny()
```

