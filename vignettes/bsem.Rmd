---
title: "bsem"
subtitle: "An R package for Bayesian structural equation modeling"
author: 
  - name          : "Renato V. Panaro"
    affiliation   : "Departamento de Estatística, IME, Universidade de São Paulo"
    corresponding : yes
    address       : "São Paulo-SP, Brazil"
    email         : "renatovp@ime.usp.br"
  - name          : "Vinicius. D. Mayrink"
    affiliation   : "Departamento de Estatística, ICEx, Universidade de Federal de Minas Gerais"
    corresponding : no
    address       : "Belo Horizonte-MG, Brazil"
    email         : "vdinizm@gmail.com"
  - name          : "Marcelo A. Costa"
    affiliation   : "Departamento de Engenharia de Produção, Escola de Engenharia, Universidade de Federal de Minas Gerais"
    corresponding : no
    address       : "Belo Horizonte-MG, Brazil"
    email         : "macosta.est@gmail.com"    

keywords          : "structural equation model, factor analysis"
date: "`r Sys.Date()`"
output:
  html_document:
    theme: paper
    toc: true
    toc_float:
      collapsed: true
      smooth_scroll: true
      toc_number: true
#bibliography: references.bib
vignette: >
  %\VignetteIndexEntry{An R package for semi-parametric survival analyis}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---

<style>
body {
text-align: justify
</style>

# Introduction

This vignette provides some instructions on how to fit particular cases of structural equation models (SEMs) using the `bsem` package. Examples include confirmatory factor analysis (CFA) and confirmatory SEM. The flexible full SEM model (outer model + inner model), implemented here, enables the evaluation of user-defined constructs (latent variables) along with the analysis of established relationships among the latent scores (factors depending on other factors). Factor analysis (FA) is a powerful and flexible tool to investigate the multivariate dependence in a data set. The main aim is to reduce dimension, i.e., summarize the information expressed in multiple observed variables. The summarization process involves the construction of few unobserved latent factors representing the existing subjacent patterns in the data. The usual FA can be divided in two types: exploratory (EFA) and confirmatory (CFA). The EFA does not impose any restriction on the relationship between latent factors and observed variables (the model is free to establish such associations). The CFA accounts for the technical information regarding the connections between observed and latent variables; examples of applications involving the CFA can be found in [Brown](https://www.guilford.com/books/Confirmatory-Factor-Analysis-for-Applied-Research/Timothy-Brown/9781462515363) (2015). In this case the researcher must impose that a factor is exclusively explained by a group of observed variables. The CFA is widely used in the context of SEM, see the references  [Lee and Song](https://link.springer.com/article/10.1007/BF02296651) (2003), [Skrondal and Rabe-Hesketh](https://www.routledge.com/Generalized-Latent-Variable-Modeling-Multilevel-Longitudinal-and-Structural/Skrondal-Rabe-Hesketh/p/book/9781584880004) (2004), [Palomo, Dunson and Bollen](https://www.elsevier.com/books/handbook-of-latent-variable-and-related-models/lee/978-0-444-52044-9) (2007), [Hoyle](https://www.guilford.com/books/Handbook-of-Structural-Equation-Modeling/Rick-Hoyle/9781462516797) (2014) and [Keith](https://www.routledge.com/Multiple-Regression-and-Beyond-An-Introduction-to-Multiple-Regression/Keith/p/book/9781138061446)(2019). Besides the EFA and CFA, it is possible to work with an intermediate case combining these two types. This option can be seen as a semi-confirmatory FA and an example is described in [Mayrink and Lucas](https://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.aoas/1372338468) (2013) and [Mayrink and Lucas](https://projecteuclid.org/euclid.bjps/1414674773) (2015).

The structure of a SEM has two parts, one involving a CFA and another with multiple regression equations. For an overview about SEMs, it is recommended to read the work [Sanchez et al.](https://www.jstor.org/stable/27590684?seq=1) (2005), which is focused on applications in epidemiology. The studies [Anderson and Gerbing](https://psycnet.apa.org/doiLanding?doi=10.1037%2F0033-2909.103.3.411) (1988) and [MacCallum and Austin](https://doi.org/10.1146/annurev.psych.51.1.201) (2000) provide an overview about the use of SEM in psychology. Examples using SEMs in the context of Bayesian inference are discussed in [Palomo, Dunson and Bollen](https://www.elsevier.com/books/handbook-of-latent-variable-and-related-models/lee/978-0-444-52044-9) (2007). A clear advantage of using the Bayesian approach is the fact that asymptotic assumptions, required by frequentist maximum likelihood methods, may not be valid depending on the sample size. The reference [Lee and Song](https://link.springer.com/article/10.1007/BF02296651) (2003) highlights the good performance of the Bayesian SEM in a comparison with the frequentist version.

Consider:

- the outer model as:
$$\boldsymbol{X_{p\times n}} = \boldsymbol{\alpha}_{p\times k}\boldsymbol{\lambda}_{k\times n} +\boldsymbol{\varepsilon}_{p\times n}$$
where $\boldsymbol{X}$ is the data matrix with variables in the rows and sample elements in the columns,  $\boldsymbol{\alpha}_{p\times j}$ is the column vector of loadings for the $j^{th}$ latent variable and $\boldsymbol{\lambda}_{j\times n}$ is the row vector of scores for the  $j^{th}$ unobserved variable$,~j =1,\dots,k$. Normality is assumed for the errors as $\boldsymbol{\varepsilon}_{ij}\sim N(0, \sigma_{i}^2)$ for $i = 1,\dots, p$.

- the inner model as:
$$\boldsymbol{\lambda}_{j\times n} = \boldsymbol{\beta}^{\top} {\lambda^{(-j)}}  + \nu~$$
where $\boldsymbol{\beta}$ is a column vector of constant coefficients, ${\lambda^{(-j)}}_{ (k-1)\times n}$ is the matrix of scores excluding the $j^{th}$ row scores and the error assumes $\nu \sim N(0,1)$.

The guidelines in this document were divided into 6 subsequent sections: [installation](#installation), [description](#description), [usage](#examples), [graphical analysis](#graphical-posterior-analysis), [descriptives](#descriptives) and [remarks](#remarks).

# Installation

The ``bsem`` package imports  ``Stan`` software specific routines to support internal calculations. The [``rstan``](https://mc-stan.org/users/interfaces/rstan) package interface enables the use  of NUTS algorithm ( [Hoffman and Gelman, 2014](http://www.stat.columbia.edu/~gelman/research/published/nuts.pdf))  to explore the posterior distribution efficiently in R. The development version of ``bsem`` can be found in [github.com/rvpanaro/bsem](https://github.com/rvpanaro/bsem).

For github version:

```{r, eval = F, echo = T}
library("devtools")
devtools::install_github("rvpanaro/bsem")
```

For CRAN version:

```{r, eval = F, echo = T}
library("devtools")
install.packages("bsem")
```

During the installation, other dependencies might be required, such as ``lattice``, ``viridis``, ``coda``, ``DiagrammeR``, ``visNetwork`` and, obviously, ``rstan``. Other packages are also suggested such as ``ggplot2``, ``bayesplot`` and ``tidybayes``. The default arguments used to build the ``bsem::sem`` routine will be introduced in the next sections this function is the main function of this package, as it allows the Bayesian analysis for SEM and its particular cases.

# Description

The data and additional commands are passed through the ``bsem::sem`` function as:

- ``data``: a mandatory 'matrix' object where the columns are variables and the rows are observations
- ``paths``: a list of characters referring to the scores relationship (inner model); list elements must be named according to ``blocks``
- ``blocks``: a list indicating the variables that might express relationship to the latent variables assigned (outer model); latent variables must be named
- ``signals``: a list indicating the variables that might express relation to the latent variable assigned (outer model)
- ``row_names``: identifier for each set of variables (observation = row); optional
- ``prior_specs``: prior specification for the error variance ``sigma2`` and regression coefficients ``beta``
- ``cores``: number of core threads to be used; arbitrary, however the quantity of cores in use does not exceed the number of chains
- ``pars``: character vector with the names of the parameters to be considered after fitting; "alpha" refers to the loadings, "lambda" to the scores, "sigma2" to the errors, "Xna" to missing data, "log_lik" to the log likelihood summation
- ``iter``: number of MCMC iterations
- ``chains``: number of chains considering distinct initial values
- ``scaled``: logical; whether to ``scale()`` the data set passed to the ``data`` argument
- ``...``: further arguments passed to ``rstan::sampling`` such as ``warmup``, ``control``

```{r, eval = F}
bsem::sem(data,
           paths,
           blocks,
           signals,
           row_names = rownames(data),
           prior_specs = list(coef = c("normal(0,1)"),
                         error_var = c("inv_gamma(2.1, 1.1)")),
           cores =  parallel::detectCores(),
           pars = c("alpha", "lambda", "sigma2", "Xna"),
           iter = 2000,
           chains = 4,
           scaled = TRUE,
          ...)
```

The above function automatically selects the appropriate routine according to the specified arguments, examples are given in the next section. One can reproduce the examples bellow by copying and pasting the following code to the ``R`` console.  For these examples, simulated artificial data was created so that we can verify that the estimates are close enough to the true parameter values used to generate the data.

# Examples

This section describes how the package can be used to estimate artificial parameters. This is a former analysis in light of other studies based on replication such as Monte Carlo simulation study methods. 

## Uni-factor CFA

```{r, include = F}
set.seed(2020)
```

This example is based on the uni-factor model ( [Reckase](https://journals.sagepub.com/doi/abs/10.3102/10769986004003207), 1979). 

- The number of variables (columns), observations (rows) and latent variables to be simulated are defined:

```{r}
Nv <- 8; Ne <- 30; K <- 1 
```
 
- A list object `B` is necessary to represent each block that indicates which manifest variables are related to each construct:
 
```{r}
 B <- list(F1 = 1:Nv); B
```

- Artificial variances are used to generate errors of a normal distribution with 0 mean:

```{r}
 sigma2 <- runif(Nv, 0.1, 0.9)

 # errors matrix
 errors <- array(0, c(Nv, Ne)) 
 for(i in 1:Nv){errors[i,] <- rnorm(Ne, 0, sqrt(sigma2[i]))}
```

- Factor scores (lambda) are generated from a standard normal distribution and the factor loadings (alpha) from a uniform distribution with support ranging from 0.5 to 2. The factor loading direction is generated using signals simulated from the `sample` function.

```{r}
lambda <- array(rnorm(K*Ne), c(K, Ne)) # Factor scores matrix
 
# Factor loadings matrix
alpha <- array(0, c(Nv, K))

for(k in 1:K){alpha[B[[k]],k] = runif(lengths(B)[k], 0.5, 2) * sample(c(-1,1), lengths(B)[k], replace = T)}

signals <- list(F1 = array(sign(alpha)))
```

- Finally, a new artificial data set is produced with the matrix product below:

```{r}
 X = t(alpha %*% lambda + errors)
```

Given that the data set was generated synthetically, we can run `bsem::sem` to check if they retrieve the true values of the parameters used in the generation of the database. We simply use the data argument and the blocks argument to specify the one-factor model:

```{r, results = F}
unifact <- bsem::sem(X, blocks = B, signals = signals, chains = 1)
```

Descriptive statistics for the posterior factor loadings are showed in the R console inspired on the `rstan` fashion (based on ``rstan::monitor``). In general, the posterior mean is the Bayesian estimator first choice:

```{r}
unifact
```

The structural model can be easily viewed with the plotting routine `bsem::plot`:

```{r}
plot(unifact)
```

Moreover, we can compare the posterior factor loadings and the scores with the true values using the `bsem::arrayplot` function. For example:

```{r}
gridExtra::grid.arrange(bsem::arrayplot(as.matrix(lambda), main = "scores (lambda)", -4,4),
                        bsem::arrayplot(as.matrix(unifact$mean_scores), main = "scores estimate", -4,4),
                        bsem::arrayplot(alpha, main = "loadings (alpha)", -4,4),
                        bsem::arrayplot(unifact$mean_loadings, main = "loadings estimate", -4,4),
                        layout_matrix = matrix(c(1,1,3,3,2,2,4,4), ncol= 2))

```

The ``bsem::arrayplot`` graphs show that the loadings and scores pattern were captured by the model. In order to intensify the contrast between the estimates it may be necessary to increase the number of iterations or the warmup period since this is a model with many parameters. Remember each chains runs in a separate thread, thus, it is recommended specifying a number of cores at least equal to the number of chains. Indeed, from our experience, an increase in the number of iterations raises the runtime but considerably improves the interval and point estimates.  

## Multi-factor CFA (uncorrelated)

Artificial data sets are part of the package content as *.rdata* files. Therefore, they can be retrieved using the specific  "character" name within the ``data()`` argument. The `set1` and` set2` examples were generated using routines similar to those shown in the uni-factor example.

```{r, results = F, message=F}
library('bsem')
```

```{r}
data("set1")
```

The implemented routines allow partial and full uncorrelated confirmatory factor analysis (CFA). So, at least one variable must be chosen to express each construct before running into the analysis. In other words, the user needs to specify at least one manifest variable that lead each block. In this way, partial EFA/CFA assists the researcher in evidencing the possible structures to be tested in a conceptual (full) CFA later on.

- The list of blocks represent that indicates only the manifest variables related to each construct:

```{r}
set1$blocks
```

- Descriptive statistics for the posterior factor loadings are showed in the result in `rstan` fashion (based on ``rstan::monitor``):

```{r, results = F}
multifact <- bsem::sem(data = set1$set, blocks = set1$blocks, chains = 1, scaled = F)
```

- The structural model can be easily viewed with the plotting routine `bsem::plot`:

```{r}
plot(multifact)
```

```{r}
gridExtra::grid.arrange(
  bsem::arrayplot(multifact$mean_scores, main = 'estimates',  mini = -2, maxi = 2),
  bsem::arrayplot(set1$real$lambda, main = 'lambda (scores)', mini = -2, maxi = 2),
  bsem::arrayplot(multifact$mean_loadings, main = 'estimates', 
                  mini = -2, maxi = 2),
  bsem::arrayplot(set1$real$alpha, main = 'alpha (loadings)',  mini = -2, maxi = 2),
  layout_matrix= matrix(c(1,1,3,3,2,2,4,4), ncol = 2))
```

The state-of-art ``bsem`` package only provides estimates for uncorrelated SEMs.  The recursive arrows indicate the estimate for each error variance. From this example, we found that the estimates pattern are tighly close to the true values of the parameters even when the signals are not passed. Also, we have observed that the part of the analysis is intended to confirm the predefined constructs CFA (first loadings matrix lines) and the other is intended to express which variable can compose a later full CFA.

## SEM

In the last example, not all variables were used in the formulation of predefined constructs. In many applications, with the help of technical knowledge, it is possible to establish relationships between all variables in the data set and the constructs under study. This case characterizes the confirmatory factor analisys (CFA). Besides that,
we can also verify linear relationships between latent variables. The blocks can be passed as named lists and the database with colnames and rownames. For example, for set2 we have:

```{r}
library('bsem')
data("set2")
```

It is worth mentioning that the paths are named with block names. There are two ways to define the model: 1) the blocks and the data set are not named previously (internally named); 2) named blocks and named paths are passed to their respective arguments; otherwise should generate an error.

The data argument, the blocks and the paths arguments should specify ta full uncorrelated SEM model. The blocks represent the manifest variables that form each construct while the paths represent the linear relationships between the latent variables:

```{r, results = F}
sem <- bsem::sem(data = set2$set, blocks = set2$blocks,
                     paths = set2$paths, signals = set2$signals,
                 chains =4, scaled = F)
```

````{r}
plot(sem)
```

```{r}
gridExtra::grid.arrange(bsem::arrayplot(sem$mean_scores, main = 'estimates', mini = -4, maxi = 4), 
                      bsem::arrayplot(set2$real$lambda, main = 'lambda (scores)', mini = -4, maxi = 4),
                      bsem::arrayplot(sem$mean_loadings, main = 'estimates', mini = -4, maxi = 4),
                      bsem::arrayplot(set2$real$alpha, main = 'alpha (loadings)', mini = -4, maxi = 4),
                      layout_matrix = matrix(c(1,1,3,3,2,2,4,4), ncol = 2))
```

Again, we found that the estimates pattern are tighly close to the true values of the parameters. This examples highlights a complete confirmatory SEM analysis. The regression coefficients estimates are:

```{r}
pander::pander(cbind(sem$coef, set2$real$beta))
```

## Model Misspecification

The data generated for the uni-factor example, is also used here. This example refers to an application of the bi-factor model ( [Jennrich and Bentler](https://link.springer.com/article/10.1007/s11336-011-9218-4), 2011; [Gibbons and Hedeker](https://link.springer.com/article/10.1007/s11336-011-9218-4), 1992). The ideia behind this application is to show how flexible is SEM. From our generated data we have that variables should manifest only the latent variable `F1`. 

- The list object `B` represents each block, indicating which manifest variables are related to each construct. `B` and, consequently, `signals` are modified for a bi-factor analysis.

```{r}
B$F2 <- 1:3 
B$F3 <- 4:6
B$F4 <- 7:8

B
```

```{r}
signals$F2 <- rep(1,3) 
signals$F3 <- rep(1,3)
signals$F4 <- rep(1,2)

signals
```


Simply use the data argument and the blocks argument to specify the bi-factor model above:

```{r, results = F}
bifact <- bsem::sem(X, blocks = B, signals = signals, chains = 1)
```

Descriptive statistics for the posterior factor loadings are showed in the result in `rstan` fashion:

The structural model is displayed using `bsem::plot`:

```{r}
plot(bifact, layout = 'nicely')
```

Compare the posterior factor loadings and the scores with the true values using the `bsem::arrayplot` function:

```{r}
gridExtra::grid.arrange(bsem::arrayplot(t(lambda), main = "scores (lambda)", -4,4),
                        bsem::arrayplot(bifact$mean_scores, main = "scores estimate", -4,4),
                        bsem::arrayplot(alpha, main = "loadings (alpha)", -4,4),
                        bsem::arrayplot(bifact$mean_loadings, main = "loadings estimate", -4,4),
                        layout_matrix = matrix(c(1,1,3,3,2,2,4,4), ncol= 2))
```

The ``bsem::arrayplot`` graphs show that the loadings and scores pattern were captured by the model. An increase in the number of iterations or the warmup period may improve the point estimates. As expected, the outcome estimates evidance a one-factor model, which means other latent variables would not be helpful in explaining this data (as we know from the generation). This conclusion can also be achieved looking at the diagram.`

# Graphical posterior analysis 

All content exposed in the examples can be retrived by the user folowing the comand lines above. Although, keep in mind that we must check posterior statistics in order to assess whether the estimates are good to describe the proposed CFA or SEM model.

## Interval estimate

Two types of intervals can be obtained:  

### HPD interval  
- The narrowest (highest posterior density - HPD) credibility interval can be retrieved using `.$credint`. We have computed HPD intervals fo the factor loadings (alpha), scores (lambda), and regression coefficients (beta).

```{r}
names(sem$credint)
```

- This data can be used to plot interval estimates using two packages, `ggplot2` and `tidybayes`:

```{r, results = F, message = F, warning= F} 
library("ggplot2")
library("tidybayes")
```

- `dt` data.frame object has the mean loadings and the HPD interval lower and upper limits (`ll` and `lu`):

```{r}
dt <- data.frame(li = sem$credint$loadings[,1],
                 lu = sem$credint$loadings[,2],
                 m = c(sem$mean_loadings))
```

- `lnames` and `snames` are used to find the loadings of the conceptual model (those that might not equal zero).

```{r}
lnames <-rownames(sem$mean_loadings)
snames <- rownames(sem$mean_scores)
```

- `find` help us finding these values:

```{r}
find <- paste0("alpha[", which(lnames %in% unlist(sem$blocks)), ",", rep(1:length(sem$blocks), lengths(sem$blocks)), "]")

dt <- dt[find,]
```

- One of the options to plot the loadings HPD intervals chart is:
```{r}
ggplot(aes(y = find, x = m, xmin = li, xmax = lu), data = dt) +
  geom_pointintervalh() + theme_classic() +
  labs(title = paste("Latent variable", colnames(sem$mean_loadings)[3]),
       x = "Highest posterior density interval",
       y = "variable")
```

In the SEM example above, all intervals regarding the loading estimates from the  conceptual relationships do not include zero. 

### Equal tails interval

Alternatively, it is possible to access equal tails credibility intevals using the `bayesplot` package:

````{r, message = F, message = F}
library("bayesplot")
```

- `find` help us finding the loading values that have been previously determined to estimate the latent scores:

```{r, eval =F}
find <- paste0("alpha[", which(lnames %in% unlist(sem$blocks)), ",", rep(1:length(sem$blocks), lengths(sem$blocks)), "]")

dt <- dt[find,]
```

- The equal tails intervals are:

````{r, warning = F}
gridExtra::grid.arrange(mcmc_areas(sem$posterior$alpha[,,find]),
                        mcmc_intervals(sem$posterior$alpha[,,find]),
                        layout_matrix = matrix(c(1,1,2,2), ncol = 2))
````

In addition to the possibilities for intervals, other `mcmc_.` type graphs are highly recommended, several options include histograms, violin plots, pair plots and others.

# Descriptives

- The `bsem::summary` routine prints descriptive statistics to the R console:

```{r}
summary(sem)
```

The effective sample size  ``n_eff`` gives an estimate of the independent draws from the posterior distribution, and ``Rhat`` referred to as the potential scale reduction statistic,  is one of the useful ways to monitor whether a chain has converged to the equilibrium distribution. This statistic measures the ratio between the average variation of the samples within each chain and the variation of the combined samples in the chains; if the chains have not converged to a common region, this statistic will be greater than one. 

- It is worth noting that, one can have acces to all the information provided by the ``Stan`` output object using the ``.$stanfit`` command:

```{r}
class(sem$stanfit) 
```

In particular, one can have access to built-in plot functions and even to a ``shiny`` app (details in [https://shiny.rstudio.com/](https://shiny.rstudio.com/) developed by ``Stan`` developer's team

For the shiny app run:

```{r, eval = F}
shinystan::launch_shinystan(sem$stanfit)
```

A crucial difference in obtaining the same statistics printed by the `bsem::summary` function is the use of the signals argument in the `bsem::sem` routine. The signal specification, (-1 or 1) for each of the factor loadings, forces the initial values of each chain to have the same direction and, naturally, these chains converge to nearby regions. Otherwise, the signal of some factor loading chain may be opposite as a consequece of random initialization. In order to circumveit this issue, the package automatically transform posterior chains using simple `for` loops and `if` statements to recalculate the descriptives using `rstan::monitor`.

- The monitoring result is returned with `.$` : 

```{r}
head(sem$stats, 2) 
```


The content of this vignette introduces the ``bsem`` package. Here, the analysis was dedicated to
illustrating, in practice, the commands implemented in the proposed package. We
still have work to do to improve and update this tool, however, the present version is ready
for the main statistical study in the field of latent variable analysis. 

# Remarks

The routines presented above are unprecedented. This document is part of the content submitted to [CRAN](https://cran.r-project.org/). The package is also in public use and available at https://github.com/rvpanaro/bsem.

